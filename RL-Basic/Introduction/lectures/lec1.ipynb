{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning \n",
    "## MLU Session 7 - Berlin \n",
    "### Lecture 1\n",
    "\n",
    "Cyrus Vahid @ Deep Engine\n",
    "\n",
    "email: cyrusmv@amazon.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Navigating in a grid\n",
    "- If you click a button the red square will start moving. Click the same button many times, and it will move faster and faster.\n",
    "- Everytime you are moving horizontally you get a wind throwing you upwards.\n",
    "- Everytime you are moving vertically you get a wind throwing you to the left.\n",
    "- Whenever the countdown reaches 0, you loose.\n",
    "- Hitting the outside walls or red obstacles would cause you to loose\n",
    "- You win when you reach the green cell and will be rewarded 1000 bonous points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"555\"\n",
       "            height=\"620\"\n",
       "            src=\"../src/gridworld-game-with-goal.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x11ffab438>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"../src/gridworld-game-with-goal.html\", height=620, width=555)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RL Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Agent: Learner or decision maker\n",
    "- Environment: What the agent interacts with. The environment could be known, unknown, or complex.\n",
    "- Policy: A set of actions from which an agent can choose. In our example: Left-Right-Up-Down\n",
    "- Action: Decisions an agent makes based on a policy in response to the environment. \n",
    "- State: The information that is used to determine what happens next.\n",
    "- Reward: A scalar feedback that after each action the environment returns. An agent's job is to maximize the cumulative reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](../images/agent-env.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reward Hypothesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- ***All*** That all of what we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward).\n",
    "for further information and discuttion check this [link](http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/rewardhypothesis.html).\n",
    "\n",
    "- We will come back to this later as the reductionist argument could have unseen consequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Markov Property\n",
    "- The future can be predicted independently from the past and only baed on present.\n",
    " - Example: Autonomous helicopter: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "from datetime import timedelta\n",
    "YouTubeVideo('VCdxqn0fcnE', width=560*1.5, height=316*1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Velocity, location, and direction of the helicopter at time t is what we need to determine where the helicopter will be in time t+1\n",
    "- The environment affects the helicopter after each action based on wind speed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Markov Property\n",
    "$ \\boldsymbol{\\large Definition:}$\n",
    "\n",
    "$$\n",
    "\\large State\\ S_t\\ is\\ Markov\\ if\\ and\\ only\\ if: \n",
    "$$\n",
    "$$\\large \n",
    "\\boxed{ùëÉ(ùëÜ_{ùë°+1}|ùëÜ_ùë°)=ùëÉ(ùëÜ_{ùë°+1}|ùëÜ_1, ‚Ä¶, ùëÜ_ùë°)}\\ \\small without\\ action\n",
    "$$\n",
    "$$or$$\n",
    "$$\\large\n",
    "\\boxed{ùëÉ(ùëÜ_{ùë°+1}|ùëÜ_ùë°, a_t)=ùëÉ(ùëÜ_{ùë°+1}|ùëÜ_1, ‚Ä¶, ùëÜ_ùë°, a_t)}\\ \\small with\\ action\n",
    "$$\n",
    "\n",
    "$$\n",
    "Or\\ more\\ intuitively:\n",
    "$$\n",
    "$$\\large \n",
    "\\boldsymbol{Future\\ is\\ independent\\ of\\ past\\ given\\ present.}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Random Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 5.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "trajectory: \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src/examples/')\n",
    "from random_walk import *\n",
    "import tkinter as tk\n",
    "rw = RandomWalk()\n",
    "for i in range(len(rw.transition_matrix)):\n",
    "    print(*rw.transition_matrix[i])\n",
    "\n",
    "vr = VisualizeRandomWalk(rw=rw)\n",
    "vr.run()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Markov Chain\n",
    "- A stochastic model, describing a sequence of possible events where probability of each event only depends on the previous state.\n",
    "- Simply put: A sequence of randome states with Markov Property\n",
    "- A finite Markov chain is the process $ X_0, X_1, X_2, ... X_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\large Definition:$\n",
    "- The state of the Markov chain at time t is the value of $X_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Example\n",
    "<img src=\"../images/markov_state.png\" alt=\"Drawing\" style=\"width: 900px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\large Definition:$\n",
    "- The state space $S$, is the set of values each $X_t$ can take. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Example\n",
    "$$\\large S=\\{0, 1, 2, 3, 4, 5, 6 , 7, 8, 9\\}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\large Definition:$\n",
    "- a trajectory of a Markov  chain is a particular set of states  for $X_0, X_1, ...X_\\tau$.\n",
    "- A trajectory $T$ at timestep $\\tau$ is referred to as $\\{s_0, s_1, s_2,..., s_\\tau\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trajectory1: [3, 4, 3, 4, 5, 6, 7, 8, 7, 8, 7, 6, 5, 4, 3, 4, 3, 4, 5, 4, 3, 2, 1, 0, 0, 0, 1, 2, 1, 0, 1, 0, 1, 2, 3, 2, 3, 2, 1, 0, 1, 2, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 2, 1, 2, 1, 2, 1, 2, 3, 4, 3, 4, 3, 4, 5, 4, 5, 4, 3, 4, 3, 4, 3, 2, 1, 0, 1, 2, 3, 2, 1, 0, 0, 1, 2, 1, 0, 1, 0, 0, 1, 0, 1, 2, 3, 2, 1, 2, 1, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 2, 3, 4, 3, 4, 3, 2, 1, 0, 1, 2, 3, 2, 3, 2, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 2, 1, 0, 1, 0, 0, 0, 0, 1, 2, 1, 2, 3, 2, 1, 2, 1, 0, 0, 0, 0, 1, 0, 1, 2, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 2, 1, 0, 1, 0, 0, 1, 2, 3, 2, 1, 2, 3, 4, 5, 6, 5, 6, 5, 6, 7, 8, 7, 6, 7, 6, 5, 6, 5, 4, 3, 2, 3, 2, 3, 2, 1, 0, 1, 2, 3, 4, 3, 4, 5, 4, 3, 4, 5, 4, 5, 6, 5, 4, 5, 4, 3, 4, 5, 4, 5, 6, 7, 6, 7, 6, 5, 6, 7, 8, 7, 6, 5, 6, 7, 6, 5, 6, 7, 6, 5, 4, 5, 4, 3, 4, 3, 4, 5, 6, 7, 6, 5, 6, 5, 4, 3, 2, 3, 2, 1, 2, 3, 2, 3, 2, 3, 2, 1, 0, 1, 2, 3, 2, 3, 2, 1, 0, 0, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 3, 4, 5, 4, 5, 6, 7, 6, 5, 6, 7, 8, 7, 8, 7, 8, 7, 8, 9]\n",
      "\n",
      "trajectory1: [1, 0, 0, 0, 1, 2, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "\n",
      "t1 @ t=5: [3, 4, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "t1, _ = rw.move()\n",
    "t2, _ = rw.move()\n",
    "print(\"trajectory1: {}\\n\".format(t1))\n",
    "print(\"trajectory1: {}\\n\".format(t2))\n",
    "print(\"t1 @ t=5: {}\".format(t1[0:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trajectory1: [(3, '->'), (4, '->'), (5, '->'), (6, '->'), (7, '<-'), (6, '->'), (5, '->'), (6, '->'), (5, '->'), (6, '->'), (5, '->'), (6, '->'), (7, '<-'), (6, '->'), (5, '->'), (6, '->'), (5, '->'), (6, '->'), (5, '->'), (4, '->'), (3, '->'), (4, '->'), (5, '->'), (4, '->'), (3, '->'), (2, '->'), (3, '->'), (4, '->'), (3, '->'), (4, '->'), (3, '->'), (4, '->'), (5, '->'), (4, '->'), (5, '->'), (4, '->'), (3, '->'), (2, '->'), (3, '->'), (4, '->'), (3, '->'), (4, '->'), (5, '->'), (4, '->'), (5, '->'), (4, '->'), (3, '->'), (2, '->'), (3, '->'), (4, '->'), (3, '->'), (4, '->'), (5, '->'), (6, '->'), (5, '->'), (6, '->'), (7, '<-'), (6, '->'), (5, '->'), (6, '->'), (7, '<-'), (8, '<-'), (7, '<-'), (8, '<-'), (9, '-')]\n",
      "\n",
      "trajectory2: [(3, '<-'), (2, '->'), (3, '<-'), (4, '->'), (5, '->'), (6, '->'), (7, '<-'), (6, '->'), (5, '->'), (4, '->'), (5, '->'), (6, '->'), (7, '<-'), (6, '->'), (7, '<-'), (6, '->'), (5, '->'), (6, '->'), (7, '<-'), (8, '->'), (9, '-')]\n",
      "\n",
      "t1 @ t=5: [(3, '->'), (4, '->'), (5, '->'), (6, '->')]\n"
     ]
    }
   ],
   "source": [
    "t1, a1 = rw.move()\n",
    "t2, a2 = rw.move()\n",
    "print(\"trajectory1: {}\\n\".format(list(zip(t1, a1))))\n",
    "print(\"trajectory2: {}\\n\".format(list(zip(t2, a2))))\n",
    "print(\"t1 @ t=5: {}\".format(list(zip(t1[0:4], a1[0:4]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\large Definition:$\n",
    "- a trajectory of a Markov  chain is a particular set of states  for $X_0, X_1, ...X_\\tau$.\n",
    "- A trajectory $T$ at timestep $\\tau$ is referred to as $\\{s_0, a_0, s_1, a_1, s_2,, a_2 ..., s_\\tau, a_\\tau\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# More Formally\n",
    "$\\large Definition:$\n",
    "A *trajectory* is a sequence of states and actions obtained by the following procedure:\n",
    "$$\\boxed{\n",
    "1.\\ The\\ initial\\ state\\ s_1\\ is\\ chosen\\ following\\ the\\ initial\\ probability\\ p(s).\\\\\n",
    "2. For t = 1,...,\\tau ,\\\\\n",
    "\\ \\ \\ \\ (a)\\ The\\ action\\ at\\ is\\ chosen\\ following\\ the\\ policy\\ \\pi(a_t|s_t).\\\\\n",
    "\\ \\ \\ \\ (b)\\ The\\ next\\ state\\ s_{t+1}\\ is\\ determined\\ according\\ to\\ the\\ transition\\ probability\\ p(s_{t+1}|s_t, a_t).\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
