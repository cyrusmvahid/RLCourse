{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning \n",
    "## MLU Session 7 - Berlin \n",
    "### Lecture 1\n",
    "\n",
    "Cyrus Vahid @ Deep Engine\n",
    "\n",
    "email: cyrusmv@amazon.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Navigating in a grid\n",
    "- If you click a button the red square will start moving. Click the same button many times, and it will move faster and faster.\n",
    "- Everytime you are moving horizontally you get a wind throwing you upwards.\n",
    "- Everytime you are moving vertically you get a wind throwing you to the left.\n",
    "- Whenever the countdown reaches 0, you loose.\n",
    "- Hitting the outside walls or red obstacles would cause you to loose\n",
    "- You win when you reach the green cell and will be rewarded 1000 bonous points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"../src/gridworld-game-with-goal.html\", height=620, width=555)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RL Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Agent: Learner or decision maker\n",
    "- Environment: What the agent interacts with. The environment could be known, unknown, or complex.\n",
    "- Policy: A set of actions from which an agent can choose. In our example: Left-Right-Up-Down\n",
    "- Action: Decisions an agent makes based on a policy in response to the environment. \n",
    "- State: The information that is used to determine what happens next.\n",
    "- Reward: A scalar feedback that after each action the environment returns. An agent's job is to maximize the cumulative reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](../images/agent-env.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reward Hypothesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- ***All*** That all of what we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward).\n",
    "for further information and discuttion check this [link](http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/rewardhypothesis.html).\n",
    "\n",
    "- We will come back to this later as the reductionist argument could have unseen consequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Markov Property\n",
    "- The future can be predicted independently from the past and only baed on present.\n",
    " - Example: Autonomous helicopter: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "from datetime import timedelta\n",
    "YouTubeVideo('VCdxqn0fcnE', width=560*1.5, height=316*1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Velocity, location, and direction of the helicopter at time t is what we need to determine where the helicopter will be in time t+1\n",
    "- The environment affects the helicopter after each action based on wind speed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Markov Property\n",
    "$ \\boldsymbol{\\large Definition:}$\n",
    "\n",
    "$$\n",
    "\\large State\\ S_t\\ is\\ Markov\\ if\\ and\\ only\\ if: \n",
    "$$\n",
    "$$\\large \n",
    "\\boxed{ğ‘ƒ(ğ‘†_{ğ‘¡+1}|ğ‘†_ğ‘¡)=ğ‘ƒ(ğ‘†_{ğ‘¡+1}|ğ‘†_1, â€¦, ğ‘†_ğ‘¡)}\\ \\small without\\ action\n",
    "$$\n",
    "$$or$$\n",
    "$$\\large\n",
    "\\boxed{ğ‘ƒ(ğ‘†_{ğ‘¡+1}|ğ‘†_ğ‘¡, a_t)=ğ‘ƒ(ğ‘†_{ğ‘¡+1}|ğ‘†_1, â€¦, ğ‘†_ğ‘¡, a_t)}\\ \\small with\\ action\n",
    "$$\n",
    "\n",
    "$$\n",
    "Or\\ more\\ intuitively:\n",
    "$$\n",
    "$$\\large \n",
    "\\boldsymbol{Future\\ is\\ independent\\ of\\ past\\ given\\ present.}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Markov Chain\n",
    "- A stochastic model, describing a sequence of possible events where probability of each event only depends on the previous state.\n",
    "- It is memoryless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/examples/')\n",
    "from random_walk_ipython_friendly import *\n",
    "import tkinter as tk\n",
    "rw = RandomWalk()\n",
    "vr = VisualizeRandomWalk(rw=rw)\n",
    "tk.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
